Metadata-Version: 2.4
Name: Scweet
Version: 4.0
Summary: Tool for scraping Tweets
Home-page: https://github.com/Altimis/Scweet
Download-URL: https://github.com/Altimis/Scweet/archive/v3.0.tar.gz
Author: Yassine AIT JEDDI
Author-email: aitjeddiyassine@gmail.com
License: MIT
Keywords: twitter,scraper,python,crawl,following,followers,twitter-scraper,tweets
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Build Tools
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE.txt
Requires-Dist: certifi
Requires-Dist: python-dotenv
Requires-Dist: urllib3
Requires-Dist: PyVirtualDisplay
Requires-Dist: requests
Requires-Dist: beautifulsoup4==4.12.3
Requires-Dist: nodriver==0.38.post1
Requires-Dist: sqlmodel>=0.0.22
Requires-Dist: SQLAlchemy>=2.0
Requires-Dist: curl_cffi
Requires-Dist: XClientTransaction
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: download-url
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Scweet v4

Scweet v4 keeps the familiar v3 public API while moving scraping toward an API-only core and introducing DB-first account provisioning backed by SQLite.

## v4 status

- v4 facade routing is active.
- Legacy public signatures remain callable.
- Preferred import: `from Scweet import Scweet`.
- Legacy import remains supported in v4.x but is deprecated: `from Scweet.scweet import Scweet`.

## Architecture summary

- Compatibility facade in `Scweet/Scweet/scweet.py` keeps legacy method signatures.
- New core modules live in `Scweet/Scweet/v4/`.
- Stateful components use local SQLite (`runs`, `accounts`, `resume_state`, manifest cache).
- Internal runtime uses async runner + in-memory task queue + account leasing.
- Tweet search scraping is API-only; nodriver is used internally only for optional cookie bootstrap via credentials.

## Installation

```bash
pip install Scweet
```

## Import policy

Preferred import (recommended for v4 usage):

```python
from Scweet import Scweet
```

Typed config import (recommended for discoverability):

```python
from Scweet import Scweet, ScweetConfig
```

Legacy import (supported in v4.x, deprecated):

```python
from Scweet.scweet import Scweet
```

## Backward compatibility (v4.x)

Compatibility guarantees include:

- Legacy class import path still works: `from Scweet.scweet import Scweet`.
- Preferred class import path available: `from Scweet import Scweet`.
- Legacy constructor args remain accepted (for example: `env_path`, `cookies`, `cookies_path`, `n_splits`, `concurrency`, `headless`).
- Legacy method signatures preserved for:
  - `scrape` / `ascrape`
  - `get_user_information` / `aget_user_information`
  - follows methods (`get_followers`, `get_following`, etc.)
- Legacy CSV filename behavior preserved (same `save_dir` / `custom_csv_name` / naming logic).

Output changes in v4 (breaking vs v3):

- `scrape` / `ascrape` now returns a `list[dict]` of raw tweet objects from the GraphQL response (`tweet_results.result`).
- CSV output is now a curated "important fields" schema (stable header) derived from those raw GraphQL tweet objects.
  - For full coverage, use JSON output (`config.output.format="json"` or `"both"`) or the returned `list[dict]`.

## Account provisioning (DB-first)

Scweet stores account records in SQLite and reuses the DB state across runs. When you provide account sources, they are imported into the DB (best-effort) and then leased to workers during scraping.

Supported account inputs:

- `.env` (single-account, legacy style)
- `accounts.txt`
- `cookies.json` (or Netscape `cookies.txt`)
- direct `cookies=` payload

`.env` key precedence (single account):

1. `AUTH_TOKEN` + `CT0` (or `CSRF`)
2. `AUTH_TOKEN` only
3. legacy credentials (`EMAIL`/`EMAIL_PASSWORD` and/or `USERNAME`/`PASSWORD`)

Phase 1 provisioning-related config knobs:

- `accounts.provision_on_init` (default `True`)
- `accounts.bootstrap_strategy` (default `"auto"`; one of `auto`, `token_only`, `nodriver_only`, `none`)
- `runtime.strict` (default `False`; if `True`, "no usable accounts" should raise instead of returning an empty result)

## Resume modes

Scweet v4 supports three resume modes under `config.resume.mode`:

- `legacy_csv`: v3-compatible resume using max CSV `Timestamp` to override `since`.
- `db_cursor`: resume from SQLite checkpoint (`since` + `cursor`) only.
- `hybrid_safe`: try DB checkpoint first, then fallback to CSV timestamp behavior.

Important compatibility rule:

- If you instantiate through the legacy import path (`from Scweet.scweet import Scweet`), resume is forced to `legacy_csv` behavior for compatibility.

## Account source formats

### `accounts.txt`

One account per line (colon-separated):

```text
username:password:email:email_password:2fa:auth_token
```

Notes:

- Blank lines and lines starting with `#` are ignored.
- Missing trailing fields are accepted.
- The auth token segment may include additional colons; parser keeps the rest as token.

### `cookies.json`

Accepted forms include:

1. List of account records.
2. Object with `accounts: [...]`.
3. Single account object.
4. Object mapping username -> cookies/account payload.

Minimal example:

```json
[
  {
    "username": "acct1",
    "cookies": {
      "auth_token": "...",
      "ct0": "..."
    }
  }
]
```

### `cookies.txt` (Netscape export)

Scweet also accepts the classic Netscape cookies.txt format (commonly exported by browsers/extensions).

Only the cookie `name` and `value` fields are used; comments/blank lines are ignored.

### Direct `cookies=` payload

Accepted forms:

- cookie dict: `{"auth_token": "...", "ct0": "..."}`
- cookie list: `[{"name": "auth_token", "value": "..."}, ...]`
- Cookie header string: `"auth_token=...; ct0=..."`
- raw auth_token string: `"..."` (convenience)
- file path string to `cookies.json` or `cookies.txt`
- JSON string containing any of the above

## Usage examples

### Recommended: build a typed config

```python
from Scweet import Scweet, ScweetConfig

cfg = ScweetConfig.from_sources(
    db_path="scweet_state.db",
    accounts_file="accounts.txt",
    cookies_file="cookies.json",  # or cookies.txt (Netscape)
    env_path=".env",
    bootstrap_strategy="auto",
    resume_mode="hybrid_safe",
    # curl_cffi fingerprint control (optional):
    # api_http_impersonate="chrome124",
    overrides={
        # Advanced knobs live here (lease TTL, cooldowns, scheduler, etc.).
        "operations": {"account_lease_ttl_s": 600},
    },
)

scweet = Scweet(config=cfg)
```

### Output format (CSV / JSON / both)

By default, `scrape/ascrape` writes a curated CSV with important tweet/user fields (stable header).

You can control output via `config.output.format` (or `Scweet.from_sources(output_format=...)`):

- `"csv"` (default): write CSV only
- `"json"`: write JSON only
- `"both"`: write CSV + JSON
- `"none"`: write no files (still returns tweets as Python objects)

When JSON output is enabled, Scweet writes a `.json` file next to the CSV path (same basename, `.json` suffix).
When `resume=True` and the JSON file already exists, Scweet appends new tweet objects to the existing JSON array.

### Scrape (v4 import path)

```python
from Scweet import Scweet

scweet = Scweet.from_sources(
    db_path="scweet_state.db",
    accounts_file="accounts.txt",
    cookies_file="cookies.json",
    resume_mode="hybrid_safe",
    overrides={"pool": {"n_splits": 8, "concurrency": 4}},
)

tweets = scweet.scrape(
    since="2026-02-01",
    until="2026-02-07",
    words=["bitcoin", "ethereum"],
    limit=200,
    save_dir="outputs",
    custom_csv_name="crypto.csv",
    resume=True,
)

# `tweets` is a list of raw tweet objects from the GraphQL response (dicts).
```

### Preload DB then scrape (manual provisioning)

```python
from Scweet import Scweet

# Disable auto-provisioning if you want an explicit "provision first" step.
scweet = Scweet.from_sources(
    db_path="scweet_state.db",
    provision_on_init=False,
)

result = scweet.provision_accounts(
    accounts_file="accounts.txt",
    cookies_file="cookies.json",
    env_path=".env",
    cookies={"auth_token": "...", "ct0": "..."},
)
print(result)  # {"processed": ..., "eligible": ...}

# DB-first reuse: re-running provisioning does not re-bootstrap accounts that already have usable auth in SQLite.
tweets = scweet.scrape(
    since="2026-02-01",
    until="2026-02-07",
    words=["openai"],
    limit=50,
    save_dir="outputs",
    custom_csv_name="tweets.csv",
)

# Strict mode: raise instead of silently returning empty results when no usable accounts exist.
strict_scweet = Scweet(
    db_path="scweet_state.db",
    config={"runtime": {"strict": True}, "accounts": {"provision_on_init": False}},
)
strict_scweet.provision_accounts(env_path=".env")
```

### Backward-compatible usage (legacy import path)

```python
from Scweet.scweet import Scweet

# Deprecated import path, still supported in v4.x.
scweet = Scweet(env_path=".env", n_splits=5, concurrency=5, headless=True)

results = scweet.scrape(
    since="2025-01-01",
    until="2025-01-07",
    words=["scweet"],
    resume=True,
)
```

## Migration and release notes

- Migration guide: `Scweet/MIGRATION_V3_TO_V4.md`
- Changelog: `Scweet/CHANGELOG.md`

## Logging

Scweet uses the standard Python `logging` module (for example: `Scweet.v4.runner`, `Scweet.v4.auth`), and does **not**
install logging handlers for you. In notebooks (and some environments), you must configure logging to see output.

### Notebook-friendly logging setup

```python
import logging
import sys

logging.basicConfig(
    level=logging.INFO,  # DEBUG/INFO/WARNING/ERROR
    format="%(asctime)s %(levelname)s %(name)s:%(lineno)d | %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
    force=True,  # important in notebooks (overrides existing handlers)
)

# Optional: make only Scweet verbose
logging.getLogger("Scweet").setLevel(logging.DEBUG)

# Optional: quiet noisy deps
logging.getLogger("urllib3").setLevel(logging.WARNING)
```

### pytest logging

pytest captures logs by default. To see logs in test runs:

```bash
pytest -q tests -s --log-cli-level=INFO
```

## Responsible use

Scweet is not affiliated with Twitter/X. Use lawfully, respect platform terms, and avoid misuse.
